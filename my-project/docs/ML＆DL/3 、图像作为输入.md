---
subtitle: CNN
---

[TOC]

---

## ä¸€ã€å·ç§¯

labelæ˜¯ä¸€ä¸ªé«˜ç»´çš„one-hot vector

- å›¾åƒæ˜¯ä¸€ä¸ªä¸‰ç»´tensorï¼ˆé•¿ã€å®½ã€é€šé“ï¼‰
- å¦‚æœæ˜¯100 Ã— 100 çš„å›¾ç‰‡ï¼Œå…¨è¿æ¥åˆ™ä¼šæœ‰ $100 Ã— 100 Ã— 3 Ã— 1000 = 3\times10^7$ ä¸ªweightéœ€è¦è°ƒæ•´ï¼Œè®¡ç®—é‡éå¸¸å¤§
- äººçœ‹å›¾ç‰‡ä¹Ÿæ˜¯æ ¹æ®å›¾ç‰‡ä¸­çš„ç‰¹å¾ï¼Œä¹Ÿè®¸å¹¶ä¸éœ€è¦æ•´å¼ å›¾ç‰‡

---

### 1ã€ç¥ç»å…ƒè§†è§’

æ¯ä¸ªneuronåªè€ƒè™‘è‡ªå·±çš„receptive fieldä¸­çš„å³å¯ï¼Œæ¯”å¦‚receptive fieldæ˜¯ $3\times3$ï¼Œé‚£ä¹ˆæ¯ä¸ªneuronåªéœ€è¦è€ƒè™‘ $3\times3\times3$ çš„ä¸€ä¸ªåˆ—å‘é‡

ç»å…¸è®¾è®¡ï¼š

- è€ƒè™‘æ‰€æœ‰é€šé“
- kernel size ï¼š$3\times3$
- ä¸€ä¸ªreceptive field ä¼šæœ‰ä¸€ç»„neuronå¯¹å…¶å“åº”
- stride = 2 â†’ overlap = 1

![receptive-field.png](../assets/images/DL/receptive-field.png)

![param-share.png](../assets/images/DL/param-share.png)

__å‚æ•°å…±äº«__ï¼šæŸäº›neuronå‚æ•°ä¸€æ ·ï¼Œæ£€æµ‹çš„æ˜¯ç›¸åŒçš„å†…å®¹

```mermaid
graph TD
%% æœ€å¤–å±‚ Fully Connected Layer
    subgraph A[Fully Connected Layer]
        style A fill:#99ccff,stroke:#000,stroke-width:1px

    %% ç¬¬äºŒå±‚ Receptive Field
        subgraph B[Receptive Field]
            style B fill:#99ff99,stroke:#000,stroke-width:1px

        %% ç¬¬ä¸‰å±‚ Parameter Sharing
            subgraph C[Parameter Sharing]
                style C fill:#ffccaa,stroke:#000,stroke-width:1px

            %% æœ€å†…å±‚ Convolutional Layer
                D[Convolutional Layer]
            end
        end
    end

```
!!! danger "ï¼ŸBias"
    biaså¾ˆå¤§ï¼Œä½†æ˜¯ç”±äºå›¾åƒè¯†åˆ«æœ¬èº«å°±å¯ä»¥æ¥å—å¼¹æ€§ï¼Œè€Œå…¨è¿æ¥ä¼šå­˜åœ¨è¿‡æ‹Ÿåˆé—®é¢˜

---

### 2ã€å·ç§¯æ ¸è§†è§’

convolutionå±‚é‡Œé¢æœ‰**å¾ˆå¤š**`filter`ï¼Œæ¯ä¸ª`filter`å°±æ˜¯$3\times3\times3$ çš„`tensor`ï¼Œå’Œå›¾ç‰‡ä¸­çš„receptive fieldåšç‚¹ä¹˜ â†’ **å¯»æ‰¾ç‰¹å¾**

æ¯ä¸ªfilterå’ŒåŸå›¾ä¸­çš„receptive fieldç‚¹æˆå¾—åˆ°çš„æ–°å›¾ç§°ä¸ºâ€œFeature Mapâ€ï¼Œæ¯”å¦‚æœ‰63ä¸ªfliterï¼Œstride = 1ï¼Œé‚£ä¹ˆç¬¬äºŒå±‚å°±ä¼šæœ‰ $3\times3\times64$ä¸ªfilter

----

å…±ç”¨å‚æ•°å°±æ˜¯filterï¼Œfilteræ‰«è¿‡æ•´å¼ å›¾ç‰‡å°±æ˜¯convolutionçš„è¿‡ç¨‹

| **ç¥ç»å…ƒè§†è§’**                   | **å·ç§¯æ ¸ï¼ˆæ»¤æ³¢å™¨ï¼‰è§†è§’**         |
| -------------------------------- | -------------------------------- |
| æ¯ä¸ªç¥ç»å…ƒåªè€ƒè™‘ä¸€ä¸ªæ„Ÿå—é‡ã€‚     | æœ‰ä¸€ç»„æ»¤æ³¢å™¨ç”¨æ¥æ£€æµ‹å±€éƒ¨å°æ¨¡å¼ã€‚ |
| ä¸åŒæ„Ÿå—é‡ä½ç½®çš„ç¥ç»å…ƒå…±äº«å‚æ•°ã€‚ | æ¯ä¸ªæ»¤æ³¢å™¨åœ¨è¾“å…¥å›¾åƒä¸Šæ»‘åŠ¨å·ç§¯ã€‚ |

---



## äºŒã€æ± åŒ–

Max/Min/Mean Poolingï¼šé€šè¿‡åœ¨çª—å£å†…é€‰å– æœ€å¤§/æœ€å°/å¹³å‡ å€¼æ¥**ç¼©å°ç‰¹å¾å›¾å°ºå¯¸**ï¼ŒåŒæ—¶ä¿ç•™å…³é”®ä¿¡æ¯ã€‚

```mermaid
graph TD
    style R fill:transparent,stroke:transparent
    R[Repeat] --> Subgraph

subgraph Subgraph[" "]
direction TB
style Subgraph fill:transparent,stroke:transparent
A[Convolution]
style A fill:#66b2ff,stroke:#000,stroke-width:1px,rx:10,ry:10
B[Pooling]
style B fill:#66cc66,stroke:#000,stroke-width:1px,rx:10,ry:10
end

A --> B
B --> dots[......]
style dots fill:transparent,stroke:transparent


```

!!! example "AlphaGo"
    CNNåº”ç”¨äºä¸‹å›´æ£‹
    
    1. **å›´æ£‹æ£‹ç›˜æ˜¯ä¸€ä¸ªäºŒç»´ç©ºé—´ï¼ˆ19x19çš„ç½‘æ ¼ï¼‰**
    
           - æ£‹ç›˜å°±åƒä¸€å¼ å›¾åƒï¼Œé»‘ç™½æ£‹å­åˆ†å¸ƒåœ¨ç½‘æ ¼ä¸Šï¼Œæ£‹å­å½¢æˆçš„å½¢çŠ¶ã€è¿æ¥ã€çœ¼ã€æ­»æ´»ç­‰éƒ½æœ‰**å±€éƒ¨å’Œå…¨å±€çš„ç©ºé—´æ¨¡å¼**ã€‚
           - CNN å–„äºå¤„ç†è¿™ç§â€œç½‘æ ¼çŠ¶â€çš„æ•°æ®ï¼Œå°±åƒå®ƒå¤„ç†å›¾åƒä¸­çš„åƒç´ å…³ç³»ä¸€æ ·ã€‚
    
    2. **å±€éƒ¨æ¨¡å¼è¯†åˆ«ï¼ˆå±€éƒ¨ç‰¹å¾æå–ï¼‰**
           - åœ¨å›´æ£‹ä¸­ï¼Œæœ‰å¾ˆå¤š**å±€éƒ¨å®šå¼**ï¼ˆæ¯”å¦‚æ˜Ÿä½å°é£ã€äºŒé—´å¤¹ã€è™å£ï¼‰æ˜¯å›´æ£‹æ‰‹ç†ŸçŸ¥çš„æˆ˜æœ¯ã€‚
    
    3. **Poolingç§»é™¤**
           - ä½†æ˜¯è¿™é‡Œç‰¹åˆ«æ³¨æ„ä¸éœ€è¦poolingï¼Œæµ…æ˜¾çš„æƒ³å°±æ˜¯éšæ„åœ°ç§»é™¤è¡Œæˆ–è€…åˆ—æ£‹å±€æ˜¯ä¸ä¸€æ ·çš„ï¼Œè€Œä¸”æœ¬æ¥å°±æ˜¯19x19å°ºå¯¸å¾ˆå°ä¸éœ€è¦ä¸‹é‡‡æ ·æ¥é™ä½å°ºå¯¸

---

## ğŸŒŸ HW03

<div class="grid cards" markdown>

- [:fontawesome-brands-git-alt: __HW03__ reference code -- â€œ__CNN__â€](https://github.com/Gerard-Devlin/NTU-EE5184/tree/main/HW03)


</div>

??? danger "Gimmicks"
    - ä»¥ä¸‹æ–¹æ³•æ˜¯åŸæœ¬HW03ç¦æ­¢ä½¿ç”¨çš„æ–¹æ³•ï¼Œä¹Ÿå°±æ˜¯ä½¿ç”¨pretrained modelï¼Œ__æ–¹æ³•ä»…ä½œå‚è€ƒ__ï¼Œè¿™é‡Œä½¿ç”¨ResNet101

    ResNet-101 æ˜¯ **Residual Neural Networkï¼ˆResNetï¼‰** çš„ä¸€ç§å˜ä½“ï¼Œä¸“é—¨ç”¨äº **å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²** ç­‰ä»»åŠ¡ã€‚å®ƒç”± **101 å±‚** æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œç»„æˆï¼Œä¸»è¦ç‰¹ç‚¹æ˜¯ **æ®‹å·®è¿æ¥ï¼ˆResidual Connectionsï¼‰**ï¼Œå¯ä»¥æœ‰æ•ˆè§£å†³ **æ·±åº¦ç½‘ç»œä¸­çš„æ¢¯åº¦æ¶ˆå¤±** å’Œ **æ¢¯åº¦çˆ†ç‚¸** é—®é¢˜ã€‚
    
    ------
    
    **1ï¸âƒ£ ResNet-101 ç»“æ„**
    
    ResNet-101 ç”± **å·ç§¯å±‚ï¼ˆConvï¼‰ã€æ‰¹å½’ä¸€åŒ–ï¼ˆBatchNormï¼‰ã€ReLU æ¿€æ´»å‡½æ•°å’Œæ®‹å·®å—ï¼ˆResidual Blocksï¼‰** ç»„æˆã€‚
     å®ƒä¸ ResNet-50 ç»“æ„ç±»ä¼¼ï¼Œä½†æœ‰ **æ›´æ·±çš„å±‚æ•°**ï¼š
    
    **2ï¸âƒ£ æ®‹å·®è¿æ¥ï¼ˆResidual Connectionï¼‰**
    
    **æ ¸å¿ƒæ€æƒ³ï¼šè·³è·ƒè¿æ¥ï¼ˆSkip Connectionï¼‰**
     åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œå±‚æ•°è¿‡å¤šä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ï¼ˆVanishing Gradientï¼‰ã€‚ResNet é€šè¿‡ **æ®‹å·®è¿æ¥** è§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚
    
    ------
    
    **3ï¸âƒ£ ä»£ç å®ç°**
    
    - éœ€è¦æ·»åŠ ä¾èµ–
    
    ```python
    import torchvision.models as models
    from torchvision.models import resnet50, ResNet50_Weights
    ```
    
    ```python
    class Classifier(nn.Module):
        def __init__(self, num_classes=11):
            super(Classifier, self).__init__()
    
            # åŠ è½½ ResNet101 é¢„è®­ç»ƒæ¨¡å‹
            self.resnet = models.resnet101(weights=ResNet50_Weights.IMAGENET1K_V1)
    
            # ä»…è§£å†» ResNet101 çš„æœ€åå‡ å±‚è¿›è¡Œè®­ç»ƒ
            for param in self.resnet.parameters():
                param.requires_grad = False
            for param in self.resnet.layer4.parameters():  # ä»…è§£å†» layer4
                param.requires_grad = True
    
            # ä¿®æ”¹å…¨è¿æ¥å±‚
            in_features = self.resnet.fc.in_features
            self.resnet.fc = nn.Sequential(
                nn.Linear(in_features, 1024),
                nn.BatchNorm1d(1024),  # åŠ é€Ÿæ”¶æ•›
                nn.ReLU(),
                nn.Dropout(0.2),  # å¢åŠ  Dropout é˜²æ­¢è¿‡æ‹Ÿåˆ
                nn.Linear(1024, 512),
                nn.BatchNorm1d(512),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(512, num_classes)
            )
    
        def forward(self, x):
            return self.resnet(x)    
    ```
!!! tip
    å› ä¸ºæ˜¯å›¾åƒå¤„ç†ï¼Œå¯ä»¥åšä¸€ä¸ª**æ•°æ®å¢å¼º**ä»¥å¢åŠ æ¨¡å‹å¼¹æ€§ï¼ˆæ³›åŒ–èƒ½åŠ›ã€é²æ£’æ€§ï¼‰ï¼Œå†ä¿®æ”¹ç½‘ç»œï¼Œè°ƒæ•´è¶…å‚æ•°ç­‰ç­‰

1ã€__æ•°æ®å¢å¼º__

```python
train_tfm = transforms.Compose([
    # Resize (height = width = 128)
    transforms.Resize((128, 128)),
    transforms.RandomHorizontalFlip(),  # éšæœºæ°´å¹³ç¿»è½¬å›¾åƒ
    transforms.RandomRotation(15),      # éšæœºæ—‹è½¬15åº¦
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # éšæœºè‰²å½©è°ƒæ•´
    transforms.RandomAffine(10, shear=10),  # éšæœºè£åˆ‡è§’åº¦
    # ToTensor() è½¬æ¢ä¸ºå¼ é‡
    transforms.ToTensor(),
])
```

2ã€__æ®‹å·®è¿æ¥ï¼ˆResNetæ¶æ„ï¼‰__

```python
class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.downsample = downsample  # ç”¨äºåŒ¹é…ç»´åº¦

    def forward(self, x):
        identity = x
        if self.downsample is not None:
            identity = self.downsample(x)  # å¤„ç†ç»´åº¦å˜åŒ–
        
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += identity  # æ®‹å·®è¿æ¥
        return F.relu(out)

class ResNetClassifier(nn.Module):
    def __init__(self):
        super(ResNetClassifier, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        
        self.layer1 = ResidualBlock(64, 128, stride=2, downsample=nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=1, stride=2, bias=False),
            nn.BatchNorm2d(128)))
        self.layer2 = ResidualBlock(128, 256, stride=2, downsample=nn.Sequential(
            nn.Conv2d(128, 256, kernel_size=1, stride=2, bias=False),
            nn.BatchNorm2d(256)))
        self.layer3 = ResidualBlock(256, 512, stride=2, downsample=nn.Sequential(
            nn.Conv2d(256, 512, kernel_size=1, stride=2, bias=False),
            nn.BatchNorm2d(512)))
        
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Sequential(
            nn.Linear(512, 1024),
            nn.ReLU(),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Linear(512, 11)
        )
        
    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.avgpool(out)
        out = torch.flatten(out, 1)
        return self.fc(out)

```

---

