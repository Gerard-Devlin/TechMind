[TOC]

---

![Problem-in-training.svg](../assets/images/DL/Problem-in-training.svg)

## 一、常见问题

- *Model Bias*
    - 模型太简单 → 重新设计模型，更有弹性（e.g. 更多feature，deep 
- *Optimization*
    - 梯度下降找到的 $Loss$ 不够低

!!! question
    到底是什么问题？ 

    - 改变模型 → 比如层数增加，loss在training set增加，说明不是overfitting不是弹性不够，而是optimization的问题

- Overfitting
    - 增加训练数据
      - 数据增强（e.g. 图片左右镜像、……
      - 限制模型选择
      - 正则化

!!! tip
    Kaggle 
    
    - Public：公开数据集
      - Private：隐藏数据集，防止为了benchmark而选择出好的模型但其实不行，所以不要用public数据集来调模型
    
    Validation 验证集：使用验证集来**挑**选合适的**模型**

---

## 二、梯度下降问题

梯度下降失败原因（e.g. local minima、saddle point……


!!! example "$Hessian$"
      $Hessian$ 矩阵是多元函数的二阶偏导数构成的方阵，在多变量微积分和优化理论中扮演着重要的角色。对于一个具有 $n$ 个变量的函数 $f(x_1, x_2, \ldots, x_n)$，其Hessian矩阵 $H(f)$ 是一个 $n \times n$ 的矩阵，矩阵中的每个元素是函数对一对变量的二阶偏导数。
    
      具体来说，Hessian矩阵 $H(f)$ 的第 $i$ 行第 $j$ 列的元素是函数 $f$ 对变量 $x_i$ 和 $x_j$ 的二阶偏导数，即：
    
      $$
      H(f)_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}
      $$
    
      对于一个二元函数 $f(x, y)$，其Hessian矩阵可以表示为：
    
    $$
    H(f) = \begin{bmatrix}
    \frac{\partial^2 f}{\partial x^2} & \frac{\partial^2 f}{\partial x \partial y} \\
    \frac{\partial^2 f}{\partial y \partial x} & \frac{\partial^2 f}{\partial y^2}
    \end{bmatrix}
    $$
    
    
    在临界点处：
    $$
    L(\theta) \approx L(\theta') + \frac{1}{2} (\theta - \theta')^T H (\theta - \theta')
    $$
    
    代入不同的$\theta$ ,
    
    - $v^THv>0\Rightarrow \text{Local minima}$；
    
      - $v^THv<0\Rightarrow \text{Local maxima}$；
    
      - 都有则是 $\text{saddle point}$
    
    性质：
    
    1. **正定性**：如果Hessian矩阵是正定的，该点是局部最小值。
       2. **负定性**：如果Hessian矩阵是负定的，该点是局部最大值。
       3. **不定性**：如果Hessian矩阵是不定的（即有正有负的特征值），那么函数在该点有一个鞍点。

---

## 三、`batch`和`momentum`

### 1、`batch`

`1 epoch` = 把所有 `batch` 运行过一遍 → 每个 `epoch` 后要shuffle

极端：

| Batch Size | 描述                                      | 更新策略                                   |
|-------------------|-------------------------------------------|--------------------------------------------|
| N（全批量）       | 在看到所有20个样本后更新                  | 每个epoch更新一次，**噪声**小             |
| 1                 | 每个样本更新一次，一个epoch更新20次        | 每个样本更新一次，**噪声**大                   |

GPU可以进行**平行运算**，所以`batch size`增加，运行时间不会很明显增加

奇怪：batch size越大，明明噪声小，但是训练结果越差 → optimization fail，可能卡在鞍点

---

### 2、`momentum`

物理世界，球从高处滚下不一定会卡在 $\text{local minima}$ 或者 $\text{saddle point}$，会继续滚动

更新步长，计算方式是上一步的移动量减去当前梯度
$$
\theta_{\text{new}} = \theta - \alpha \left( v + \beta \nabla L(\theta) \right) 
$$
其中：

- $\theta$ 是当前的参数向量。
- $\theta_{\text{new}}$ 是更新后的参数向量。
- $\alpha$ 是学习率。
- $v$ 是动量项，通常是上一步梯度的指数加权平均。
- $\beta$ 是动量系数，控制着动量项的影响程度。
- $\nabla L(\theta)$ 是当前梯度。

---

## 四、自适应学习率

