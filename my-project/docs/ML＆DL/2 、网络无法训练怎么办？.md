---
subtitle: Classification
---

[TOC]

---

![Problem-in-training.svg](../assets/images/DL/Problem-in-training.svg)

## ä¸€ã€å¸¸è§é—®é¢˜

- *Model Bias*
    - æ¨¡å‹å¤ªç®€å• â†’ é‡æ–°è®¾è®¡æ¨¡å‹ï¼Œæ›´æœ‰å¼¹æ€§ï¼ˆe.g. æ›´å¤šfeatureï¼Œdeep 
- *Optimization*
    - æ¢¯åº¦ä¸‹é™æ‰¾åˆ°çš„ $Loss$ ä¸å¤Ÿä½

!!! question
    åˆ°åº•æ˜¯ä»€ä¹ˆé—®é¢˜ï¼Ÿ 

    - æ”¹å˜æ¨¡å‹ â†’ æ¯”å¦‚å±‚æ•°å¢åŠ ï¼Œlossåœ¨training setå¢åŠ ï¼Œè¯´æ˜ä¸æ˜¯overfittingä¸æ˜¯å¼¹æ€§ä¸å¤Ÿï¼Œè€Œæ˜¯optimizationçš„é—®é¢˜

- Overfitting
    - å¢åŠ è®­ç»ƒæ•°æ®
      - æ•°æ®å¢å¼ºï¼ˆe.g. å›¾ç‰‡å·¦å³é•œåƒã€â€¦â€¦
      - é™åˆ¶æ¨¡å‹é€‰æ‹©
      - æ­£åˆ™åŒ–

!!! tip
    Kaggle 
    
    - Publicï¼šå…¬å¼€æ•°æ®é›†
      - Privateï¼šéšè—æ•°æ®é›†ï¼Œé˜²æ­¢ä¸ºäº†benchmarkè€Œé€‰æ‹©å‡ºå¥½çš„æ¨¡å‹ä½†å…¶å®ä¸è¡Œï¼Œæ‰€ä»¥ä¸è¦ç”¨publicæ•°æ®é›†æ¥è°ƒæ¨¡å‹
    
    Validation éªŒè¯é›†ï¼šä½¿ç”¨éªŒè¯é›†æ¥**æŒ‘**é€‰åˆé€‚çš„**æ¨¡å‹**

---

## äºŒã€æ¢¯åº¦ä¸‹é™é—®é¢˜

æ¢¯åº¦ä¸‹é™å¤±è´¥åŸå› ï¼ˆe.g. local minimaã€saddle pointâ€¦â€¦


??? example "$Hessian$"
      $Hessian$ çŸ©é˜µæ˜¯å¤šå…ƒå‡½æ•°çš„äºŒé˜¶åå¯¼æ•°æ„æˆçš„æ–¹é˜µï¼Œåœ¨å¤šå˜é‡å¾®ç§¯åˆ†å’Œä¼˜åŒ–ç†è®ºä¸­æ‰®æ¼”ç€é‡è¦çš„è§’è‰²ã€‚å¯¹äºä¸€ä¸ªå…·æœ‰ $n$ ä¸ªå˜é‡çš„å‡½æ•° $f(x_1, x_2, \ldots, x_n)$ï¼Œå…¶HessiançŸ©é˜µ $H(f)$ æ˜¯ä¸€ä¸ª $n \times n$ çš„çŸ©é˜µï¼ŒçŸ©é˜µä¸­çš„æ¯ä¸ªå…ƒç´ æ˜¯å‡½æ•°å¯¹ä¸€å¯¹å˜é‡çš„äºŒé˜¶åå¯¼æ•°ã€‚
    
      å…·ä½“æ¥è¯´ï¼ŒHessiançŸ©é˜µ $H(f)$ çš„ç¬¬ $i$ è¡Œç¬¬ $j$ åˆ—çš„å…ƒç´ æ˜¯å‡½æ•° $f$ å¯¹å˜é‡ $x_i$ å’Œ $x_j$ çš„äºŒé˜¶åå¯¼æ•°ï¼Œå³ï¼š
    
      $$
      H(f)_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}
      $$
    
      å¯¹äºä¸€ä¸ªäºŒå…ƒå‡½æ•° $f(x, y)$ï¼Œå…¶HessiançŸ©é˜µå¯ä»¥è¡¨ç¤ºä¸ºï¼š
    
    $$
    H(f) = \begin{bmatrix}
    \frac{\partial^2 f}{\partial x^2} & \frac{\partial^2 f}{\partial x \partial y} \\
    \frac{\partial^2 f}{\partial y \partial x} & \frac{\partial^2 f}{\partial y^2}
    \end{bmatrix}
    $$


    
    åœ¨ä¸´ç•Œç‚¹å¤„ï¼š
    $$
    L(\theta) \approx L(\theta') + \frac{1}{2} (\theta - \theta')^T H (\theta - \theta')
    $$
    
    ä»£å…¥ä¸åŒçš„$\theta$ ,
    
    - $v^THv>0\Rightarrow \text{Local minima}$ï¼›
    
      - $v^THv<0\Rightarrow \text{Local maxima}$ï¼›
    
      - éƒ½æœ‰åˆ™æ˜¯ $\text{saddle point}$
    
    æ€§è´¨ï¼š
    
    1. **æ­£å®šæ€§**ï¼šå¦‚æœHessiançŸ©é˜µæ˜¯æ­£å®šçš„ï¼Œè¯¥ç‚¹æ˜¯å±€éƒ¨æœ€å°å€¼ã€‚
       2. **è´Ÿå®šæ€§**ï¼šå¦‚æœHessiançŸ©é˜µæ˜¯è´Ÿå®šçš„ï¼Œè¯¥ç‚¹æ˜¯å±€éƒ¨æœ€å¤§å€¼ã€‚
       3. **ä¸å®šæ€§**ï¼šå¦‚æœHessiançŸ©é˜µæ˜¯ä¸å®šçš„ï¼ˆå³æœ‰æ­£æœ‰è´Ÿçš„ç‰¹å¾å€¼ï¼‰ï¼Œé‚£ä¹ˆå‡½æ•°åœ¨è¯¥ç‚¹æœ‰ä¸€ä¸ªéç‚¹ã€‚

---

## ä¸‰ã€`batch`å’Œ`momentum`

### 1ã€`batch`

`1 epoch` = æŠŠæ‰€æœ‰ `batch` è¿è¡Œè¿‡ä¸€é â†’ æ¯ä¸ª `epoch` åè¦shuffle

æç«¯ï¼š

| Batch Size | æè¿°                                      | æ›´æ–°ç­–ç•¥                                   |
|-------------------|-------------------------------------------|--------------------------------------------|
| Nï¼ˆå…¨æ‰¹é‡ï¼‰       | åœ¨çœ‹åˆ°æ‰€æœ‰20ä¸ªæ ·æœ¬åæ›´æ–°                  | æ¯ä¸ªepochæ›´æ–°ä¸€æ¬¡ï¼Œ**å™ªå£°**å°             |
| 1                 | æ¯ä¸ªæ ·æœ¬æ›´æ–°ä¸€æ¬¡ï¼Œä¸€ä¸ªepochæ›´æ–°20æ¬¡        | æ¯ä¸ªæ ·æœ¬æ›´æ–°ä¸€æ¬¡ï¼Œ**å™ªå£°**å¤§                   |

GPUå¯ä»¥è¿›è¡Œ**å¹³è¡Œè¿ç®—**ï¼Œæ‰€ä»¥`batch size`å¢åŠ ï¼Œè¿è¡Œæ—¶é—´ä¸ä¼šå¾ˆæ˜æ˜¾å¢åŠ 

å¥‡æ€ªï¼šbatch sizeè¶Šå¤§ï¼Œæ˜æ˜å™ªå£°å°ï¼Œä½†æ˜¯è®­ç»ƒç»“æœè¶Šå·® â†’ optimization failï¼Œå¯èƒ½å¡åœ¨éç‚¹

---

### 2ã€`momentum`

ç‰©ç†ä¸–ç•Œï¼Œçƒä»é«˜å¤„æ»šä¸‹ä¸ä¸€å®šä¼šå¡åœ¨ $\text{local minima}$ æˆ–è€… $\text{saddle point}$ï¼Œä¼šç»§ç»­æ»šåŠ¨

æ›´æ–°æ­¥é•¿ï¼Œè®¡ç®—æ–¹å¼æ˜¯ä¸Šä¸€æ­¥çš„ç§»åŠ¨é‡å‡å»å½“å‰æ¢¯åº¦
$$
\theta_{\text{new}} = \theta - \alpha \left( v + \beta \nabla L(\theta) \right) 
$$
å…¶ä¸­ï¼š

- $\theta$ æ˜¯å½“å‰çš„å‚æ•°å‘é‡ã€‚
- $\theta_{\text{new}}$ æ˜¯æ›´æ–°åçš„å‚æ•°å‘é‡ã€‚
- $\alpha$ æ˜¯å­¦ä¹ ç‡ã€‚
- $v$ æ˜¯åŠ¨é‡é¡¹ï¼Œé€šå¸¸æ˜¯ä¸Šä¸€æ­¥æ¢¯åº¦çš„æŒ‡æ•°åŠ æƒå¹³å‡ã€‚
- $\beta$ æ˜¯åŠ¨é‡ç³»æ•°ï¼Œæ§åˆ¶ç€åŠ¨é‡é¡¹çš„å½±å“ç¨‹åº¦ã€‚
- $\nabla L(\theta)$ æ˜¯å½“å‰æ¢¯åº¦ã€‚

---

## å››ã€è‡ªé€‚åº”å­¦ä¹ ç‡

- æœ‰æ—¶å€™ $Loss$ æ— æ³•å†ä¸‹é™ä¸æ˜¯å› ä¸ºå¡åœ¨critical pointï¼Œè€Œæ˜¯å¯èƒ½**å­¦ä¹ ç‡**é€‰æ‹©ä¸åˆé€‚è€Œå¯¼è‡´ä¸èƒ½ä¸‹é™ï¼Œè¿™æ—¶å€™å¯ä»¥é€šè¿‡è§‚å¯Ÿgradient çš„normæ¥åˆ¤æ–­æ˜¯ç”±äºå¡åœ¨critical pointè¿˜æ˜¯**å­¦ä¹ ç‡**é€‰æ‹©ä¸åˆé€‚
    - **å­¦ä¹ ç‡**é€‰æ‹©è¿‡å¤§ â†’ è¶Šè¿‡æå°å€¼è€Œæ— æ³•æŠµè¾¾
    - **å­¦ä¹ ç‡**é€‰æ‹©è¿‡å° â†’ åœ¨gradientå¹³ç¼“çš„åœ°æ–¹éš¾ä»¥æ¢¯åº¦ä¸‹é™

$\text{åŸå§‹ï¼š}\theta_i^{t+1} \leftarrow \theta_i^t - \eta g_i^t$ï¼Œ$g_i^t = \frac{\partial L}{\partial \theta_i} \Big|_{\theta = \theta^t}$

$\text{æ”¹è¿›ï¼š}\theta_i^{t+1} \leftarrow \theta_i^t - \frac{\eta}{\sigma_i^t} g_i^t$

!!! question "æ€ä¹ˆç®—å‡º $\sigma$ï¼Ÿ"

### 1ã€Root Mean Square
ä¸»è¦ç”¨äºå¹³è¡¡æ¢¯åº¦çš„å°ºåº¦ï¼Œé˜²æ­¢æ¢¯åº¦å¤ªå¤§æˆ–å¤ªå°ï¼Œä½†å®ƒæœ¬èº«ä¸æ˜¯ä¸€ä¸ªä¼˜åŒ–ç®—æ³•ï¼Œåªæ˜¯ä¸€ä¸ªåº¦é‡æˆ–è§„èŒƒåŒ–æ‰‹æ®µï¼Œå¸¸è§äºæ¯”å¦‚ AdaGrad è¿™ç§æ–¹æ³•ã€‚

$$
\theta_i^{1} \leftarrow \theta_i^{0} - \frac{\eta}{\sigma_i^{0}} g_i^{0} \quad\quad \sigma_i^{0} = \sqrt{(g_i^{0})^2} = |g_i^{0}|
$$

$$
\theta_i^{2} \leftarrow \theta_i^{1} - \frac{\eta}{\sigma_i^{1}} g_i^{1} \quad\quad \sigma_i^{1} = \sqrt{\frac{1}{2} \left[ (g_i^{0})^2 + (g_i^{1})^2 \right]}
$$

$$
\theta_i^{3} \leftarrow \theta_i^{2} - \frac{\eta}{\sigma_i^{2}} g_i^{2} \quad\quad \sigma_i^{2} = \sqrt{\frac{1}{3} \left[ (g_i^{0})^2 + (g_i^{1})^2 + (g_i^{2})^2 \right]}
$$

$$
\vdots
$$

$$
\theta_i^{t+1} \leftarrow \theta_i^{t} - \frac{\eta}{\sigma_i^{t}} g_i^{t} \quad\quad \sigma_i^{t} = \sqrt{ \frac{1}{t+1} \sum_{i=0}^{t} (g_i^{t})^2 }
$$

- å¡åº¦å¹³ç¼“ â†’ ç®—å‡ºçš„gradientå° â†’ $\sigma$ å° â†’ $\frac{\eta}{\sigma}$ å¤§ â†’ ä¸‹é™çš„å¿«

---

### 2ã€RMSProp
æ˜¯ä¸ºäº†è§£å†³ AdaGrad ä¸­â€œå­¦ä¹ ç‡è¡°å‡å¤ªå¿«â€çš„é—®é¢˜è€Œæå‡ºçš„ï¼ŒRMSProp è®©ä¼˜åŒ–å™¨æ›´å¤šåœ°å…³æ³¨ â€œå½“å‰å’Œæœ€è¿‘çš„æ¢¯åº¦â€ï¼Œè€Œä¸æ˜¯ â€œæ‰€æœ‰å†å²çš„æ¢¯åº¦â€ï¼Œè¿™æ ·å¯ä»¥ä¿æŒä¸€ä¸ªåˆç†çš„å­¦ä¹ ç‡ï¼Œä¸ä¼šè®©å­¦ä¹ ç‡å› ä¸ºå†å²æ¢¯åº¦ç§¯ç´¯è€Œå¿«é€Ÿå˜å¾—å¾ˆå°ã€‚

$$
\theta_i^{1} \leftarrow \theta_i^{0} - \frac{\eta}{\sigma_i^{0}} g_i^{0} \quad\quad \sigma_i^{0} = \sqrt{(g_i^{0})^2} \quad\quad 0 < \alpha < 1
$$

$$
\theta_i^{2} \leftarrow \theta_i^{1} - \frac{\eta}{\sigma_i^{1}} g_i^{1} \quad\quad \sigma_i^{1} = \sqrt{ \alpha (\sigma_i^{0})^2 + (1 - \alpha)(g_i^{1})^2 }
$$

$$
\theta_i^{3} \leftarrow \theta_i^{2} - \frac{\eta}{\sigma_i^{2}} g_i^{2} \quad\quad \sigma_i^{2} = \sqrt{ \alpha (\sigma_i^{1})^2 + (1 - \alpha)(g_i^{2})^2 }
$$

$$
\vdots
$$

$$
\theta_i^{t+1} \leftarrow \theta_i^{t} - \frac{\eta}{\sigma_i^{t}} g_i^{t} \quad\quad \sigma_i^{t} = \sqrt{ \alpha (\sigma_i^{t-1})^2 + (1 - \alpha)(g_i^{t})^2 }
$$

![RMSProp.png](../assets/images/DL/RMSProp.png)

---

### 3ã€Adam

$=\text{RMSProp + Momentum}$

```python
optimizer = optim.Adam(model.parameters(), lr=0.01)
```

---

### 4ã€Learning rate Scheduling

- å­¦ä¹ ç‡è¡°å‡ï¼ˆLearning rate decayï¼‰
    - éšæ—¶é—´å¢åŠ å‡å°å­¦ä¹ ç‡ $\eta$
- é¢„çƒ­ï¼ˆWarm upï¼‰
    - $\eta$ å…ˆå¢å¤§åå‡å°

??? info
    **ç»“åˆå…³ç³»å›¾**
    
    $\text{æœ€ç»ˆæ›´æ–°å…¬å¼} = \underbrace{\eta}_{\text{å…¨å±€}} \times \underbrace{\text{è°ƒåº¦å› å­}(\text{scheduler})}_{\text{éšepochè°ƒæ•´}} \times \underbrace{\text{è‡ªé€‚åº”æ¯”ä¾‹}(\text{å¦‚} \frac{1}{\sigma_i})}_{\text{å±€éƒ¨ï¼ŒRMS/Adam}}$
    
    **æ€»ç»“å®ƒä»¬çš„å…³ç³»**ï¼š
    
    |                          | åŠŸèƒ½                     | å½±å“çš„å±‚é¢             | ä½œç”¨                                     |
    | ------------------------ | ------------------------ | ---------------------- | ---------------------------------------- |
    | RMSProp/Adam             | è‡ªé€‚åº”è°ƒæ•´ä¸åŒå‚æ•°çš„æ­¥é•¿ | **å‚æ•°å±‚é¢ï¼ˆå±€éƒ¨ï¼‰**   | é¿å…æŸäº›å‚æ•°å› æ¢¯åº¦è¿‡å¤§æˆ–è¿‡å°è€Œæ›´æ–°ä¸ç¨³å®š |
    | Learning Rate Scheduling | éšè®­ç»ƒè¿›åº¦è°ƒæ•´æ•´ä½“å­¦ä¹ ç‡ | **ä¼˜åŒ–å™¨çš„å…¨å±€å­¦ä¹ ç‡** | ä¿è¯è®­ç»ƒåˆæœŸå¤§èƒ†æœç´¢ï¼ŒåæœŸç²¾ç»†æ”¶æ•›       |

---

## äº”ã€åˆ†ç±»

### 1ã€äºŒå…ƒåˆ†ç±»

$h$ æ˜¯äºŒå…ƒåˆ†ç±»åˆ†å‰²çš„é˜ˆå€¼ï¼Œ$h^{train}$ æ˜¯è®­ç»ƒé›†ä¸Šæ‰¾åˆ°çš„æœ€å¥½é˜ˆå€¼ï¼Œ$h^{all}$ æ˜¯æ€»ä½“ä¸Šæœ€å¥½çš„é˜ˆå€¼

æˆ‘ä»¬å¸Œæœ› $L(h^{train}, D_{all}) - L(h^{all}, D_{all}) \leq \delta$

ä»€ä¹ˆæ ·çš„ $D_{train}$ èƒ½æ»¡è¶³è¿™ä¸ªæ¡ä»¶ï¼Ÿ

- â†’ å¯¹äº**æ‰€æœ‰**çš„ $h \in \mathcal{H}$ï¼Œéƒ½æœ‰ $|L(h, D_{train}) - L(h, D_{all})| \leq \delta / 2$

??? example "Derivation"
    $$
    \begin{aligned}
    L(h^{train}, D_{all}) \leq L(h^{train}, D_{train}) + \frac{\delta}{2} \\ \leq L(h^{all},D_{train}) + \frac{\delta}{2} \\ \leq L(h^{all},D_{all}) +\frac{\delta}{2} + \frac{\delta}{2} \\= L(h^{all},D_{all}) + \delta
    \end{aligned}
    $$

Hoeffdingâ€™s Inequality:

$$
P(\mathcal{D}_{train} \text{ is bad due to } h) \leq 2\exp(-2N\varepsilon^2)
$$

æ‰€ä»¥å¢å¤§$N$ï¼Œæˆ–è€…å‡å° $\mathcal{H}$ æ¥å‡å°é€‰åˆ°ä¸å¥½çš„è®­ç»ƒèµ„æ–™çš„å‡ ç‡

!!! bug
    æ›´å°çš„ $\mathcal{H}$ï¼Œæ›´å¤§çš„$L(H^{all},D_{all})$ï¼ˆç†æƒ³å´©åï¼‰ï¼Œé±¼ä¸ç†ŠæŒä¸å¯å…¼å¾—

---
### 2ã€å¤šå…ƒåˆ†ç±»

![Classification.png](../assets/images/DL/Classification.png)

![Softmax.png](../assets/images/DL/Softmax.png)

```mermaid
graph LR
    x[x] --> Network[Network]
    Network --> y[y]
    y -->|softmax| y_prime[y']
    y_prime -.->|e| y_hat[Å·]

```

- Mean Square Error (MSE)
    - $e = \sum_i (\hat{y}_i - y'_i)^2$
    - **MSE** åœ¨è¾“å‡ºæ¥è¿‘ç›®æ ‡æ—¶ï¼ˆæ¯”å¦‚ softmax è¾“å‡ºæ¥è¿‘ 0 æˆ– 1 æ—¶ï¼‰ï¼Œæ¢¯åº¦ä¼šå˜å°ï¼Œå¯¼è‡´å­¦ä¹ é€Ÿåº¦å˜æ…¢ï¼ˆå°¤å…¶åœ¨æ·±åº¦ç½‘ç»œä¸­å®¹æ˜“é™·å…¥ã€Œæ¢¯åº¦æ¶ˆå¤±ã€ï¼‰ã€‚

- Cross-entropy â†’ æ›´å¸¸ç”¨äº**åˆ†ç±»**ä»»åŠ¡
    - $e = -\sum_i \hat{y}_i \ln y'_i$

!!! tip
    PyTorchä¸­Cross-entropyå·²ç»å’Œsoftmaxç»‘åœ¨ä¸€èµ·ä¸ç”¨å†å†™softmax

---

## å…­ã€æ‰¹æ¬¡æ ‡å‡†åŒ–

### 1ã€è®­ç»ƒ
![FeatureNorm.png](../assets/images/DL/FeatureNorm.png)

ä¸¤æ¬¡æ ‡å‡†åŒ–ï¼š

- è¾“å…¥æ ‡å‡†åŒ–
- $\times w_i$ ä¹‹åå†æ ‡å‡†åŒ–ï¼ˆå¯ä»¥åœ¨è¿›å…¥æ¿€æ´»å‡½æ•°å‰ï¼Œä¹Ÿå¯ä»¥åœ¨ä¹‹åï¼‰

![BatchNorm.png](../assets/images/DL/BatchNorm.png)

!!! bug
    è®­ç»ƒæ—¶ï¼Œä¸Šç™¾ä¸‡ç¬”èµ„æ–™æ— æ³•å…¨éƒ¨è¿›GPUï¼Œåªèƒ½è€ƒè™‘ä¸€ä¸ª `mini batch` çš„æ•°æ®

---

### 2ã€æµ‹è¯•

**æµ‹è¯•é˜¶æ®µ**ï¼Œä¸€èˆ¬æ˜¯é€ä¸ªæ ·æœ¬æµ‹è¯•ï¼Œæˆ–è€… batch å¾ˆå°ï¼Œè¿™æ—¶å€™å†å»è®¡ç®— $\mu$ å’Œ $\sigma$ ä¼šä¸ç¨³å®šã€‚

â†’ **è§£å†³æ–¹æ¡ˆï¼šæ»‘åŠ¨å¹³å‡ï¼ˆMoving Averageï¼‰**

æ‰€ä»¥ï¼Œåœ¨è®­ç»ƒé˜¶æ®µï¼ŒBN è¿˜ä¼š**ç»´æŠ¤ä¸€ä¸ªå…¨å±€çš„æ»‘åŠ¨å¹³å‡**ï¼ˆmoving averageï¼‰ï¼š

$\bar{\mu} \leftarrow p \cdot \bar{\mu} + (1-p) \cdot \mu^t$

- $\bar{\mu}$ æ˜¯**å…¨å±€çš„å‡å€¼ä¼°è®¡**ï¼ˆå†å²å€¼ï¼‰
- $\mu^t$ æ˜¯å½“å‰ batch çš„å‡å€¼
- $p$ æ˜¯å¹³æ»‘å› å­ï¼Œé€šå¸¸ $p$ æ¯”å¦‚æ˜¯ **0.9 æˆ– 0.99**

---


## ğŸŒŸ HW02
<div class="grid cards" markdown>

- [:fontawesome-brands-git-alt: __HW02__ reference code -- â€œ__Classification__â€](https://github.com/Gerard-Devlin/NTU-EE5184/tree/main/HW02)

</div>

!!! tip
    æ€è·¯ç…§æ—§ï¼šä»ç„¶æ˜¯è°ƒæ•´ __ç½‘ç»œç»“æ„__ï¼Œè°ƒæ•´è¶…å‚æ•°ã€ä¼˜åŒ–å™¨

1ã€__Model__

```python
class BasicBlock(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(BasicBlock, self).__init__()
        self.block = nn.Sequential(
            nn.Linear(input_dim, output_dim),  # çº¿æ€§å±‚ï¼Œå°†è¾“å…¥ç»´åº¦æ˜ å°„åˆ°è¾“å‡ºç»´åº¦
            nn.ReLU(),  # ReLU æ¿€æ´»å‡½æ•°ï¼Œæ·»åŠ éçº¿æ€§å˜æ¢
            nn.BatchNorm1d(output_dim),  # æ‰¹é‡å½’ä¸€åŒ–ï¼Œæ ‡å‡†åŒ–æ¯ä¸ªå°æ‰¹é‡çš„è¾“å…¥
            nn.Dropout(0.25),  # Dropout å±‚ï¼Œéšæœºä¸¢å¼ƒ 25% çš„ç¥ç»å…ƒï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        )

        def forward(self, x):  # å‰å‘ä¼ æ’­æ–¹æ³•ï¼Œå®šä¹‰å¦‚ä½•è®¡ç®—è¾“å‡º
            x = self.block(x)  # å°†è¾“å…¥ x é€šè¿‡ self.block ä¼ é€’
            return x  # è¿”å›ç»è¿‡å±‚å †å åçš„è¾“å‡º
```

2ã€__Hyperparameter__

```python
# æ•°æ®å‚æ•°
concat_nframes = 21              # è¦æ‹¼æ¥çš„å¸§æ•°ï¼Œn å¿…é¡»æ˜¯å¥‡æ•°ï¼ˆæ€»å…± 2k+1 = n å¸§ï¼‰ï¼Œéœ€è¦è°ƒæ•´ä»¥è·å¾—æ›´é«˜çš„å¾—åˆ†
```

