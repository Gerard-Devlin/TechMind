[TOC]

---

!!! tip
    本文是对于机器学习基础概念的简单回顾，具体可以参照**机器学习（ML）**的文档

## 一、通用步骤

- *step1*：找出 $f$ 来**拟合**

- *step2*：从训练函数定义 $Loss$ 

    - $$
      \begin{aligned}
      MAE:e = |y - \hat{y}| \\
       MSE:e = (y - \hat{y})^2
       \end{aligned}
      $$

-  *step3*：**梯度下降**寻找最优解（局部）

    - $$
      w^*,b^*=\arg \min_{w,b} L
      $$

!!! note
    **超参数**：需要自己设定的参数，e.g. 激活函数、batch、epoch ……

---

## 二、激活函数

线性模型 $y=\vec w·\vec x+b$ 不能拟合复杂情况，需要更有**弹性**的函数

用足够多的激活函数凑出piecewise linear，再用足够多的piecewise linear来逼近原来的曲线
$$
\begin{align}
y=c·\frac{1}{1+e^{-(b+wx_1)}}\\
=c·sigmoid(b+wx_1)
\end{align}
$$
通过调整 $w,b,c$ 来组合出不同的 $sigmoid$ 函数

- 进而改进模型
  $$
  f=y = b + \sum_i c_i \, \text{sigmoid} \left( b_i + \sum_j w_{ij} x_j \right)
  $$
  

![model.png](../assets/images/DL/model.png)

---

- 改进 $Loss$

  将参数展开成列向量 $\theta$，然后 $\theta^* = \arg\min_{\theta} L$


$$
\theta = \begin{bmatrix}
\theta_1 \\
\theta_2 \\
\theta_3 \\
\vdots
\end{bmatrix}
$$


  - 随机初始化 $\theta^0$


$$
\quad g_{gradient} = \begin{bmatrix}
\frac{\partial L}{\partial \theta_1} \bigg|_{\theta=\theta^0} \\
\frac{\partial L}{\partial \theta_2} \bigg|_{\theta=\theta^0} \\
\vdots
\end{bmatrix}
=\nabla L(\theta^0)
$$
  

!!! tip

    把数据分成多个 `batch`，`1 epoch` = 把所有 `batch` 运行过一遍
    
    - e.g. N = 10,000， B = 10，每个`epoch`更新1,000 次
    
    通过增加网络层数可以显著降低 $Loss$
    
    Deep Learning 中 Deep 代表有许多的隐藏层，但是光通过叠层数会导致**过拟合**（overfitting）的问题

---

## 三、反向传播

$$
L(\theta) = \sum_{n=1}^{N} c^n(\theta)\Rightarrow
\frac{\partial L(\theta)}{\partial w} = \sum_{n=1}^{N} \frac{\partial c^n(\theta)}{\partial w}
$$

- 前向传播（Forward pass）
    - 计算所有参数的偏导数 $\frac{\partial z}{\partial w}$。


- 反向传播（Backward pass）

    - 计算所有激活函数输入 $z$ 的偏导数 $\frac{\partial C}{\partial z}$。

    - 简单讲核心思想就是：

      **反向传播是通过链式法则，将损失对输出的误差，逐层反向传播，计算各层参数的梯度，从而用来更新参数，降低损失。**

![backward-pass.png](../assets/images/DL/backward-pass.png)

![forward-pass.png](../assets/images/DL/forward-pass.png)