---
subtitle: Self-attention
---

[TOC]

---

## ä¸€ã€å¼•å…¥

è¾“å…¥æ˜¯ä¸€ä¸ªå‘é‡å¯ä»¥ç”¨ä¹‹å‰çš„æ¨¡å‹è§£å†³ï¼Œä½†å‡å¦‚è¾“å…¥æ˜¯ä¸€æ’å‘é‡è€Œä¸”æ•°é‡ä¸å›ºå®šï¼Œè¿™æ—¶å€™æ€ä¹ˆåŠï¼Ÿ

### 1ã€è¾“å…¥

é—®é¢˜ï¼š è¾“å…¥ä¸€å¥å¥å­ï¼Œæ¯ä¸ªè¯å¯ä»¥çœ‹ä½œä¸€ä¸ªå‘é‡ï¼Œå¦‚ä½•ç¼–ç ï¼Ÿ

è§£å†³æ–¹æ³•ï¼š

- One-hot Encodingï¼šå¯ä»¥å¼€ä¸€ä¸ªå¾ˆé•¿çš„one-hot vectorï¼Œä½†æ˜¯å‘é‡**æ²¡æœ‰**ä»»ä½•**è¯­ä¹‰**ä¿¡æ¯
- Word Embeddingï¼šå«æœ‰è¯­ä¹‰ä¿¡æ¯

!!! example
    e.g. ä¸€æ®µå£°éŸ³ä¿¡å·ï¼Œå–25msä¸ºä¸€ä¸ªframeï¼Œç”¨ä¸€ä¸ªå‘é‡æè¿°ä¸€ä¸ªframeä¸­çš„ä¿¡æ¯ï¼Œæ¯æ¬¡å‘å³æ»‘åŠ¨10msï¼Œæ‰€ä»¥1så«æœ‰100ä¸ªframe
    
    e.g. å›¾ï¼ˆGraphï¼‰å¯ä»¥è¢«çœ‹ä½œæ˜¯ä¸€ç»„å‘é‡ã€‚è¿™é‡Œçš„â€œå‘é‡â€æŒ‡çš„æ˜¯å›¾çš„æ¯ä¸€ä¸ªèŠ‚ç‚¹ï¼Œæ¯ä¸ªèŠ‚ç‚¹éƒ½å¯ä»¥è¢«è§†ä¸ºä¸€ä¸ªå‘é‡ã€‚

---

### 2ã€è¾“å‡º

- **ä¸€ä¸ªè¾“å…¥æœ‰ä¸€ä¸ªè¾“å‡º**ã€‚æ¯ä¸ªè¾“å…¥æ•°æ®ç‚¹ï¼ˆå‘é‡ï¼‰éƒ½æœ‰ä¸€ä¸ªå¯¹åº”çš„è¾“å‡ºæˆ–ç±»åˆ«ï¼ˆæ ‡ç­¾ï¼‰
    - e.g. è¯æ€§æ ‡æ³¨ï¼ˆPOS taggingï¼‰ï¼ŒSocial Network
- **ä¸€æ•´ä¸ªåºåˆ—åªæœ‰ä¸€ä¸ªè¾“å‡º**
    - e.g. æƒ…æ„Ÿåˆ†æï¼ˆSentiment Analysisï¼‰ï¼Œè¯­è€…è¾¨è¯†ï¼ˆSpeaker  recognitionï¼‰
- **ä¸çŸ¥é“è¦è¾“å‡ºå‡ ä¸ªç±»åˆ«ï¼ˆæ ‡ç­¾ï¼‰**
    - e.g. `seq2seq`ç¿»è¯‘ï¼Œè¯­éŸ³è¾¨è¯†

---

### 3ã€åºåˆ—æ ‡æ³¨

åºåˆ—æ ‡æ³¨ï¼ˆsequence labellingï¼‰ï¼šå°±æ˜¯**ç»™åºåˆ—ä¸­çš„æ¯ä¸ªå…ƒç´ æ‰“æ ‡ç­¾**ï¼Œä¸€èˆ¬ä¼š**å‚è€ƒå‰åä¸€å®šèŒƒå›´ï¼ˆçª—å£ï¼‰**çš„æ•°æ®æ¥åšå†³å®šï¼Œæ¯”å¦‚ç”¨ BiLSTM æˆ– Self-Attention å¤„ç†ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

!!! bug
    å…¨è¿æ¥ï¼šæ¯”å¦‚è¯æ€§æ ‡æ³¨ï¼Œe.g. I saw a saw. ä¸€å¥è¯ä¸­åŒä¸€ä¸ªè¯è¯æ€§è¾“å‡ºä¸€å®šä¸€æ ·

    â†’ æ”¹è¿›ï¼šè€ƒè™‘ä¸€ä¸ªwindowçš„vector

!!! question "æ€ä¹ˆè€ƒè™‘æ•´ä¸ªåºåˆ—ï¼Ÿ"  
    â†’ è‡ªæ³¨æ„åŠ›æœºåˆ¶

---

## äºŒã€è‡ªæ³¨æ„åŠ›

![self-attention.png](../assets/images/DL/self-attention.png)

<div class="grid cards" markdown>

- [:fontawesome-brands-medapps:__Attention__ Is All You Need](https://arxiv.org/abs/1706.03762)

</div>


---

### 1ã€æœºåˆ¶

- å¯»æ‰¾è¯ä¹‹é—´çš„å…³è”æ€§ $\alpha$
    - æ¯ä¸ªè¯å½“ä½œqueryå’Œå…¶ä»–è¯å½“ä½œkeyåšdot-productï¼Œç®—å‡ºattention scoreï¼ˆè‡ªå·±å’Œè‡ªå·±ä¹Ÿè¦ç®—å…³è”æ€§ï¼‰ï¼Œä¹‹ååšä¸€ä¸ª`Soft-max`ï¼ˆä¸ä¸€å®šæ˜¯`Soft-max`ï¼Œ`ReLU`ä¹Ÿå¯ä»¥ï¼‰

![self-attention-2.png](../assets/images/DL/self-attention-2.png)

![self-attention-3.png](../assets/images/DL/self-attention-3.png)

![self-attention-4.png](../assets/images/DL/self-attention-4.png)

---

#### çŸ©é˜µä¹˜æ³•è§’åº¦

$$
\text{Attention}(Q, K, V) = \text{softmax} \left(\frac{QK^T}{\sqrt{d_k}}\right) V
$$

å…¶ä¸­ï¼š

- $Q$ï¼ˆQueryï¼‰ï¼šæŸ¥è¯¢çŸ©é˜µï¼Œè¡¨ç¤ºå½“å‰è¯çš„æŸ¥è¯¢å‘é‡
- $K$ï¼ˆKeyï¼‰ï¼šé”®çŸ©é˜µï¼Œè¡¨ç¤ºæ•´ä¸ªåºåˆ—ä¸­çš„é”®å‘é‡
- $V$ï¼ˆValueï¼‰ï¼šå€¼çŸ©é˜µï¼Œè¡¨ç¤ºæ•´ä¸ªåºåˆ—ä¸­çš„å€¼å‘é‡
- $d_k$ï¼šå‘é‡çš„ç»´åº¦ï¼Œ$\sqrt{d_k}$ ç”¨äºæ•°å€¼ç¨³å®šæ€§

![self-attention-5.png](../assets/images/DL/self-attention-5.png)

---

### 2ã€å¤šå¤´è‡ªæ³¨æ„åŠ›

**å¤šå¤´æœºåˆ¶ï¼ˆMuti-head Self-attentionï¼‰**çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šè®©æ¨¡å‹å¯ä»¥å­¦ä¹ å¤šä¸ªä¸åŒçš„æ³¨æ„åŠ›æ¨¡å¼ï¼Œä»ä¸åŒçš„è§’åº¦ç†è§£è¾“å…¥æ•°æ®ã€‚
$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O
$$


å…¶ä¸­ï¼š

- $\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$
- $W_i^Q, W_i^K, W_i^V$ æ˜¯ä¸åŒå¤´çš„æŠ•å½±çŸ©é˜µ
- $W^O$ æ˜¯æœ€ç»ˆè¾“å‡ºçš„æŠ•å½±çŸ©é˜µ

![self-attention-6.png](../assets/images/DL/self-attention-6.png)


??? info
    - ä½ç½®ç¼–ç  :åœ¨æ¯ä¸ª $a^i$ ä¸ŠåŠ ä¸Šä¸€ä¸ª $e^i$ï¼Œ$e^i$ æ˜¯äººå®šçš„ï¼ˆå°šå¾…ç ”ç©¶ï¼‰
    
    - Self-attention åº”ç”¨ï¼š
    
        - è¯­éŸ³ï¼štruncated self-attention
    
        - å›¾ç‰‡ï¼šæŠŠå›¾ç‰‡çœ‹ä½œvector set â†’ é‚£ä¹ˆCNNå’ŒSelf-attentionéƒ½å¯ä»¥å¤„ç†å›¾ç‰‡æœ‰ä»€ä¹ˆåŒºåˆ«å‘¢ï¼Ÿ
---

### 3ã€å¯¹æ¯”CNN/RNN

!!! danger "Comparison"

    æ¯”è¾ƒâ‘  ï¼šSelf-attention v.s CNN
        
    - CNNå¯ä»¥çœ‹ä½œç®€åŒ–ç‰ˆçš„Self-attentionï¼Œå› ä¸ºæ¯ä¸ªneuronåªè€ƒè™‘receptive field
    
    - CNNåœ¨èµ„æ–™å°‘çš„æ—¶å€™æ•ˆæœå¥½ï¼ŒSelf-attentionåœ¨èµ„æ–™å¤šçš„æ—¶å€™æ•ˆæœå¥½ã€‚CNNå¼¹æ€§å°ï¼Œèµ„æ–™å¤šå®¹æ˜“è¿‡æ‹Ÿåˆï¼›Self-attentionå¼¹æ€§å¤§ï¼Œèµ„æ–™å¤šäº†åè€Œæ•ˆæœå¥½
    
    ---
    
    æ¯”è¾ƒâ‘¡ ï¼šSelf-attention v.s RNN
    
    ![self-attention-7.png](../assets/images/DL/self-attention-7.png)

---

## ğŸŒŸ HW04

<div class="grid cards" markdown>

- [:fontawesome-brands-git-alt: __HW04__ reference code -- â€œ__Self-attention__â€](https://github.com/Gerard-Devlin/NTU-EE5184/tree/main/HW04)

</div>

!!! tip
    æ€è·¯ç…§æ—§ï¼šå…ˆè°ƒæ•´æ¨¡å‹ï¼ˆæ„å»ºConformeræ¶æ„ï¼Œå¤šä¸€ä¸ªconvolutional moduleï¼‰ï¼Œå†è°ƒæ•´è¶…å‚æ•°

- å¯ä»¥å°è¯• `self-attention pooling` ï¼Œ `additive margin softmax`ï¼Œè¿˜æœ‰ä½¿ç”¨ __`conformer`__

??? info "Dataset"
    - æ–‡ä»¶ç»“æ„å¦‚ä¸‹
      ``` bash
      \- data directory 
       |---- metadata.json # `"n_mels"`ï¼šæ¢…å°”è°±çš„ç»´åº¦ã€‚
       |                   # `"speakers"`ï¼šä¸€ä¸ªå­—å…¸ã€‚**é”®**ï¼šè¯´è¯äºº IDï¼›**å€¼**ï¼šåŒ…å« "ç‰¹å¾è·¯å¾„" å’Œ "æ¢…å°”é•¿åº¦"
       |---- testdata.json # ä¿¡æ¯åŸºæœ¬åŒä¸Š
       |---- mapping.json  # â€œspeaker2idâ€: æŒ‡æ˜äº†æ¼”è®²è€…å’Œ id çš„å¯¹åº”å…³ç³»ï¼Œä¾‹: id00464": 0, â€œid00559â€: 1, â€œid00578â€: 2, â€œid00905â€: 3
       |---- uttr-{random string}.pt # è¯­éŸ³ç‰‡æ®µ
      ```

---

![conformer.png](../assets/images/DL/conformer.png)
